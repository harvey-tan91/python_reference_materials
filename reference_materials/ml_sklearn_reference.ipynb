{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***SkLearn Reference***\r\n",
    "=======================\r\n",
    "\r\n",
    "* [Dimensionality Reduction](#Dimensionality-Reduction)\r\n",
    "\r\n",
    "**Standard Import**\r\n",
    "-------------------\r\n",
    "```\r\n",
    "import sklearn.model_selection as ms\r\n",
    "import sklearn.impute as impute\r\n",
    "import sklearn.preprocessing as pp\r\n",
    "import sklearn.pipeline as pp\r\n",
    "import sklearn.compose as compose\r\n",
    "import sklearn.decomposition as decom\r\n",
    "\r\n",
    "import sklearn.metrics as metrics \r\n",
    "```\r\n",
    "---\r\n",
    "<br>\r\n",
    "\r\n",
    "**Processing Data**\r\n",
    "-------------------\r\n",
    "\r\n",
    "*Handle Missing Data*\r\n",
    "---------------------\r\n",
    "[API - simpleimputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\r\n",
    "\r\n",
    "```\r\n",
    "imputer = impute.SimpleImputer(strategy = {'mean', 'median', 'most_frequent', 'constant', fill_value= 10, copy= True)\r\n",
    "imputer.fit_transform(X) --- return a numpy array\r\n",
    "```\r\n",
    "\r\n",
    "*Transformation*\r\n",
    "----------------\r\n",
    "[API - preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)<br>\r\n",
    "[API - polyfeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)<br>\r\n",
    "[API - minmax](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)<br>\r\n",
    "[API - standard](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)<br>\r\n",
    "[API - oneHot](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)<br>\r\n",
    "[API - getdummies](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)\r\n",
    "\r\n",
    "```\r\n",
    "pp.PolynomialFeatures(degree, include_bias)\r\n",
    "pp.MinMaxScaler()\r\n",
    "pp.StandardScaler()\r\n",
    "\r\n",
    "one_hot = pp.OneHotEncoder(sparse= False) --- return a numpy array if sparse = False else return a sparse matrix\r\n",
    "one_hot.get_feature_names() --- return the feature names in the transformed data\r\n",
    "\r\n",
    "pd.get_dummies(X) --- one-hot encode\r\n",
    "```\r\n",
    "\r\n",
    "*Splitting Data*\r\n",
    "----------------\r\n",
    "[API - train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\r\n",
    "\r\n",
    "```\r\n",
    "ms.train_test_split(X, y, train_size, random_state, shuffle, stratify)\r\n",
    "```\r\n",
    "\r\n",
    "*Data Pipeline*\r\n",
    "---------------\r\n",
    "[API - pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\r\n",
    "[API - columntransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\r\n",
    "\r\n",
    "```\r\n",
    "data_pipe = pipe.Pipeline([\r\n",
    "                            ( 'scaler', pp.StandardScaler() ), \r\n",
    "                            ( 'impute', impute.SimpleImputer(strategy='median') ),\r\n",
    "                            ( 'tree_classifier', tree.DecisionTreeClassifier(random_state=42) )\r\n",
    "                            ])\r\n",
    "----------\r\n",
    "num_pipeline = pipe.Pipeline([\r\n",
    "                        ( 'impute', impute.SimpleImputer(strategy='mean') ),\r\n",
    "                        ( 'scaler', pp.StandardScaler() ),\r\n",
    "\t\t\t\t\t\t\t])\r\n",
    "\t\t\t\t\t\t\t\t\r\n",
    "full_pipeline = compose.ColumnTransformer([\r\n",
    "                        ( 'num', num_pipeline, lst_num ),\r\n",
    "                        ( 'cat', pp.OneHotEncoder(), lst_cat ),\r\n",
    "                        ], remainder='passthrough')\r\n",
    "```\r\n",
    "* lst_num is the list variable containing all the variable name of the columns to be transformed.\r\n",
    "\r\n",
    "```\r\n",
    "full_pipeline.fit_transform(X)\r\n",
    "full_pipeline.named_transformers_['cat'].get_feature_names()\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "<br>\r\n",
    "\r\n",
    "**Dimensionality Reduction**\r\n",
    "----------------------------\r\n",
    "* Remember to scale the data before applying dimensionality reduction on the data\r\n",
    "\r\n",
    "```\r\n",
    "import sklearn.decomposition as decom\r\n",
    "```\r\n",
    "[API - pca](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)<br>\r\n",
    "[API - incrementalpca](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)<br>\r\n",
    "[API - kernelpca](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)\r\n",
    "\r\n",
    "| Type            \t| Require the data to fit in memory? \t| Notes                                                                                                                            \t|\r\n",
    "|-----------------\t|------------------------------------\t|----------------------------------------------------------------------------------------------------------------------------------\t|\r\n",
    "| Regular PCA     \t| Yes                                \t|                                                                                                                                  \t|\r\n",
    "| Randomized PCA  \t| Yes                                \t| * Quickly find an approximation of the first d principal components <br>    * Faster then full SVD when d is much smaller than n \t|\r\n",
    "| Incremental PCA \t| No                                 \t| * Good for large training set that cannot be fitted into memory <br> * Good for applying PCA online                              \t|\r\n",
    "| Kernel PCA      \t|                                    \t| * For complex nonlinear projections                                                                                              \t|\r\n",
    "\r\n",
    "\r\n",
    "<strong>Regular PCA</strong>\r\n",
    "\r\n",
    "```\r\n",
    "pca = decom.PCA(n_components=int/float, svd_solver= 'auto') --- to create a PCA object\r\n",
    "```\r\n",
    "* \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "---\r\n",
    "<br>\r\n",
    "\r\n",
    "**Classifiers**\r\n",
    "---------------\r\n",
    "\r\n",
    "*Linear Classifiers*\r\n",
    "--------------------\r\n",
    "```\r\n",
    "import sklearn.linear_model as lm\r\n",
    "```\r\n",
    "<strong>Logistic Regression</strong>\r\n",
    "\r\n",
    "[API - logisticregression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\r\n",
    "\r\n",
    "```\r\n",
    "lmLogisticRegression() --- this is a native binary classifier\r\n",
    "lm.LogisticRegression(\r\n",
    "        multi_class=\"multinomial\", \r\n",
    "        solver=\"lbfgs\", --- must be stated for a softmax regression \r\n",
    "        C=10, --- applied regularization, inverse C aka higher value lowwer regularization\r\n",
    "        ) --- this is a multiclass classifier aka softmax regression\r\n",
    "\r\n",
    "estimator.fit(X, y)\r\n",
    "estimator.predict(X)\r\n",
    "estimator.predict_proba(X) --- return probability estimates\r\n",
    "estimator.decison_function(X)\r\n",
    "estimator.score(X, y) --- return the mean accuracy\r\n",
    "```\r\n",
    "\r\n",
    "<strong>SGD Classifier</strong>\r\n",
    "\r\n",
    "[API - sgdclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\r\n",
    "\r\n",
    "```\r\n",
    "lm.SGDClassifier() --- this is a native multi-class classifier\r\n",
    "```\r\n",
    "\r\n",
    "*NN Classifiers*\r\n",
    "----------------\r\n",
    "```\r\n",
    "import sklearn.neural_network as nn\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Multi-Layer Perceptron  Classifier</strong>\r\n",
    "\r\n",
    "[API - mlpclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\r\n",
    "\r\n",
    "```\r\n",
    "nn.MLPClassifier(\r\n",
    "        hidden_layer_sizes= 10, \r\n",
    "        activation= {'identity', 'logistic', 'tanh', 'relu'}, \r\n",
    "        solver= {'lbfgs', 'sgd', 'adam'}\r\n",
    "        )\r\n",
    "```\r\n",
    "\r\n",
    "*Tree Classifiers*\r\n",
    "------------------\r\n",
    "```\r\n",
    "import sklearn.tree as tree\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Decision Tree Classifier</strong>\r\n",
    "\r\n",
    "[API - dtclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\r\n",
    "[API - plottree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)\r\n",
    "\r\n",
    "```\r\n",
    "tree.DecisionTreeClassifier(\r\n",
    "        max_depth= 10,\r\n",
    "        cp_alpha= [0,1],\r\n",
    "        max_featurs,\r\n",
    "        max_leaf_nodes,\r\n",
    "        min_samples_split,\r\n",
    "        min_sample_leaf\r\n",
    "        )\r\n",
    "\r\n",
    "estimator.cost_complexity_pruning_path(X, y) --- generate path that contain alpha (pruning values) and impurity score\r\n",
    "\r\n",
    "tree.plot_tree(estimator, \r\n",
    "                filled=True, \r\n",
    "                rounded=True, \r\n",
    "                class_name=['negaitve_class', 'positive_class'], \r\n",
    "                ) --- to generate tree diagram \r\n",
    "\r\n",
    "```\r\n",
    "\r\n",
    "*Ensemble Classifiers*\r\n",
    "----------------------\r\n",
    "```\r\n",
    "import sklearn.ensemble as ensemble\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Random Forest Classifier</strong>\r\n",
    "\r\n",
    "[API - rfclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\r\n",
    "\r\n",
    "```\r\n",
    "ensemble.RandomForestClassifier(\r\n",
    "       \t\tn_estimators,\r\n",
    "\t\tbootstrap= True\r\n",
    "\t\tmax_samples,\r\n",
    "                oob_score, ---if True, uses out-of-bag samples to estimate the generalization accuracy\r\n",
    "                n_jobs ---> number of CPU cores to use for training and prediction\r\n",
    "                )     \r\n",
    "```\r\n",
    "\r\n",
    "<strong>Voting Classifier</strong>\r\n",
    "\r\n",
    "[API - votingclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)\r\n",
    "\r\n",
    "```\r\n",
    "ensemble.VotingClassifier(\r\n",
    "                estimators=[ ('name', estimator),.... ],\r\n",
    "                voting= {'hard', 'soft'},\r\n",
    "                flatten_transform= {True, False}\r\n",
    "                )\r\n",
    "```\r\n",
    "| flatten_transform \t| voting \t| What it return after calling transform method on X                      \t|\r\n",
    "|-------------------\t|--------\t|-------------------------------------------------------------------------\t|\r\n",
    "| True              \t| hard   \t| Return predicted class label                                            \t|\r\n",
    "| {False, True}     \t| soft   \t| Return the probabilities of class labels for all estimators in ensemble \t|\r\n",
    "\r\n",
    "<strong>Bagging Classifier</strong>\r\n",
    "\r\n",
    "[API - baggingclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\r\n",
    "\r\n",
    "```\r\n",
    "ensemble.BaggingClassifier(\r\n",
    "        base_estimator, \r\n",
    "        n_estimators, \r\n",
    "        max_samples, \r\n",
    "        max_features, \r\n",
    "        bootstrap, \r\n",
    "        bootstrap_features, \r\n",
    "        oob_score\r\n",
    "        )\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Ada Boost Classifier</strong>\r\n",
    "\r\n",
    "[API - adaboostclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\r\n",
    "\r\n",
    "```\r\n",
    "ensemble.AdaBoostClassifier(\r\n",
    "        base_estimator, \r\n",
    "        n_estimators=50, \r\n",
    "        learning_rate=1.0, \r\n",
    "        algorithm='SAMME.R\r\n",
    "        )\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Gradient Boosting Classifier</strong>\r\n",
    "\r\n",
    "[API - gbclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\r\n",
    "\r\n",
    "```\r\n",
    "ensemble.GradientBoostingClassifier()\r\n",
    "```\r\n",
    "\r\n",
    "*SVM Classifiers*\r\n",
    "-----------------\r\n",
    "```\r\n",
    "import sklearn.svm as svm \r\n",
    "```\r\n",
    "* SVM classifiers do not output probabilities \r\n",
    "* SVM classifiers are sensitive to feature scales so remember to scale the features before fitting them \r\n",
    "\r\n",
    "\r\n",
    "<strong>Linear Data</strong>\r\n",
    "\r\n",
    "[API - svc](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\r\n",
    "[API - linearsvc](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\r\n",
    "\r\n",
    "```\r\n",
    "svm.SVC() --- this is a native binary classifier that uses OvO strategy natively\r\n",
    "\r\n",
    "svm.SVC(\r\n",
    "        kernel='linear', \r\n",
    "        C=1, \r\n",
    "        probability=False)\r\n",
    "```\r\n",
    "\r\n",
    "* This is the same as the LinearSVC class. \r\n",
    "* If probability = True, it will add predict_proba() and predict_log_proba() to the estimator's method.\r\n",
    "\r\n",
    "```\r\n",
    "svm.LinearSVC(C=1, loss='hinge') ---- this is linear support vector classifier\r\n",
    "```\r\n",
    "* C is the inverse C, applied regularization, higher the value, the less applied regularization\r\n",
    "* The LinearSVC class regularize the bias term as well and hence we need to standardized the training set \r\n",
    "* If we scale the data during the preprocessing stage, it will not be an issue \r\n",
    "* Remember to set the loss = 'hinge' as it is not the default value \r\n",
    "\r\n",
    "<strong>Non-Linear Data</strong>\r\n",
    "\r\n",
    "```\r\n",
    "svm.SVC(\r\n",
    "        kernel='poly' , \r\n",
    "        degree=2, \r\n",
    "        coef0=1, \r\n",
    "        C=5)\r\n",
    "```\r\n",
    "* This trains a SVM classifier with a int-degree polynomial kernel \r\n",
    "* It gets the same results as if we have added polynomial features without actually having to add them\r\n",
    "* coef0 controls how much the model is influenced by high-degree poly model vs low-degree poly model    \r\n",
    "* When degree=int, the poly kernel computes the relationship between each pair observations in int-dimension and the relationship is used to find the threshold/SVC\r\n",
    "\r\n",
    "```\r\n",
    "svm.SVC(\r\n",
    "        kernel='rbf', \r\n",
    "        gamma=int, \r\n",
    "        C=0.001)\r\n",
    "```\r\n",
    "* This train a SVM classifier with the RBF kernel \r\n",
    "* gamma / C are inverse applied regularization, higher the value, the less applied regularization\r\n",
    "\r\n",
    "\r\n",
    "<strong>Computational Complexity</strong>\r\n",
    "\r\n",
    "| Class                    \t| Kernel Trick \t| Time                                                                                                                                                                                                                                                                                                  \t| Out of Core Support \t|\r\n",
    "|--------------------------\t|--------------\t|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|---------------------\t|\r\n",
    "| LinearSVC <br> LinearSVR \t| No           \t| * Scale   linearly with number of training instances and number of features O(m x n) <br> * m ~ number of instances <br> * n ~ number of features                                                                                                                                                     \t| No                  \t|\r\n",
    "| SVC <br>  SVR            \t| Yes          \t| * When the number of training instances get large, it gets very slow <br> * O(m^2 x n) to O(M^3 x n) <br> * Good for complex small to medium datasets datasets with large number of features <br> * Especially for dataset with sparse features (i.e. with very few nonzero features per instance)    \t| No                  \t|\r\n",
    "| SGDClassifier            \t| No           \t| * Scale linearly with number of training instances and number of features <br> * O(m x n)                                                                                                                                                                                                     \t|     Yes             \t|\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "\r\n",
    "*Strategy Conversion*\r\n",
    "---------------------\r\n",
    "\r\n",
    "```\r\n",
    "import sklearn.multiclass as mc\r\n",
    "```\r\n",
    "[API - 1v1classifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html)\r\n",
    "[API - 1vRclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html)\r\n",
    "\r\n",
    "```\r\n",
    "mc.OneVsOneClassifier() \r\n",
    "mc.OneVsRestClassifier(svm.SVC()) --- to use SVC as a multiclass classifier\r\n",
    "```\r\n",
    "\r\n",
    "\r\n",
    "---\r\n",
    "<br>\r\n",
    "\r\n",
    "**Regression**\r\n",
    "--------------\r\n",
    "```\r\n",
    "import sklearn.linear_model as lm\r\n",
    "```\r\n",
    "\r\n",
    "*Base Models*\r\n",
    "-------------\r\n",
    "\r\n",
    "[API - linearregression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\r\n",
    "[API - sgdregression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\r\n",
    "\r\n",
    "```\r\n",
    "lm.LinearRegression()\r\n",
    "lm.SGDRegressor()\r\n",
    "```\r\n",
    "\r\n",
    "*Tree Regressors*\r\n",
    "-----------------\r\n",
    "```\r\n",
    "import sklearn.tree as tree\r\n",
    "```\r\n",
    "\r\n",
    "[API - decisiontreeregression](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\r\n",
    "[API - randomforestregression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\r\n",
    "\r\n",
    "```\r\n",
    "tree.DecisionTreeRegressor()\r\n",
    "ensemble.RandomForestRegressor() \r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "<br>\r\n",
    "\r\n",
    "**Clustering**\r\n",
    "--------------\r\n",
    "\r\n",
    "```\r\n",
    "import sklearn.cluster as cluster\r\n",
    "```\r\n",
    "* Remember to scale the data before performing clustering on the data\r\n",
    "* Might be insightful to plot the inertia and silhouette score as a function of k (number of clusters)\r\n",
    "\r\n",
    "```\r\n",
    "kms = cluster.Kmeans(\r\n",
    "        n_cluster=8, --- number of cluster \r\n",
    "        n_init=10,  --- number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia\r\n",
    "        init={'k-means++', 'random', array}, --- methods to select the cluster center during the initialization process\r\n",
    "        algorithm={'auto'....} --- type of algorithm to run \r\n",
    "        )\r\n",
    "\r\n",
    "kms.inertia --- to generate the inertia score (mean squared distance between the instances and its closest centroid)\r\n",
    "kms.cluster_centers_ --- return the centroids coordinates\r\n",
    "kms.labels_ --- return the instance's label (index of the cluster that this instance get assigned to by the algorithm)\r\n",
    "kms.transform(X) --- return the distance between each instance and all of the clusters\r\n",
    "\r\n",
    "mb_kms = cluster.MiniBatchKmeans(n_cluster=int.....)\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "<br>\r\n",
    "\r\n",
    "**Metrics for Model Evaluation**\r\n",
    "--------------------------------\r\n",
    "[API - modelevaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\r\n",
    "\r\n",
    "```\r\n",
    "import sklearn.metrics as metrics\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Regression</strong>\r\n",
    "\r\n",
    "```\r\n",
    "metrics.mean_squared_error(y_actual, y_prediction) --- to generate the mean squared error\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Classification</strong>\r\n",
    "\r\n",
    "```\r\n",
    "metrics.confusion_matrix(y_actual, y_prediction) \r\n",
    "metrics.plot_confusion_matrix(estimator, X, y, display_labels])\r\n",
    "\r\n",
    "metrics.precision_score(y_actual, y_prediction)\r\n",
    "metrics.recall_score(y_actual, y_prediction)\r\n",
    "metrics.f1_score(y_actual, y_prediction)\r\n",
    "metrics.roc_auc_score(y_actual, y_prediction)\r\n",
    "metrics.classification_report(y_actual, y_prediction)\r\n",
    "\r\n",
    "metrics.precision_recall_curve(\r\n",
    "                        y_actual, \r\n",
    "                        y_prediction_score) --- generate range of precision, recall values at different thresholds\r\n",
    "metrics.roc_curve(\r\n",
    "        y_actual, \r\n",
    "        y_prediction_score) --- generate range of FPR, TPR at different thresholds\r\n",
    "\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Cross-Validationn</strong>\r\n",
    "\r\n",
    "[API - crossvalscore](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)<br>\r\n",
    "[API - crossvalpredict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html)\r\n",
    "\r\n",
    "```\r\n",
    "ms.cross_val_score(estimator, X, y, cv=10,\r\n",
    "                        scoring={'accuracy', 'neg_mean_squared_error'} )\r\n",
    "\r\n",
    "ms.cross_val_predict(estimator, X, y, cv=10,\r\n",
    "                        method={'decision_function', 'predict_proba'} ) \r\n",
    "\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "<br>\r\n",
    "\r\n",
    "**Grid Search and Randomized Search**\r\n",
    "-------------------------------------\r\n",
    "\r\n",
    "<strong>Grid Search</strong>\r\n",
    "\r\n",
    "[API - gridsearchcv](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\r\n",
    "\r\n",
    "```\r\n",
    "parameter_grid= [ \r\n",
    "                {'n_estimators' :[2,4,6,8], 'max_features': [2,4,6]}, --- tests all permutations in dictionary \r\n",
    "                {defined parameters} ]\r\n",
    "\r\n",
    "\r\n",
    "grid_search = ms.GridSearchCV(estimator, parameter_grid, \r\n",
    "                                scoring= 'xxxx', cv=10, \r\n",
    "                                return_train_score= True)\r\n",
    "\r\n",
    "grid_search.fit(X, y)\r\n",
    "grid_search.best_estimator_\r\n",
    "grid_seach.best_estimator_.feature_importances_\r\n",
    "grid_search.best_score_\r\n",
    "grid_search.best_params_\r\n",
    "\r\n",
    "grid_search.cv_results_\r\n",
    "grid_search.cv_results_['mean_train_score'] --- to return the average training score\r\n",
    "grid_search.cv_results_['mean_test_score'] --- to return the average test score\r\n",
    "grid_search.cv_results_['params'] --- to return the parameters of the results\r\n",
    "```\r\n",
    "\r\n",
    "<strong>Randomized Search</strong>\r\n",
    "\r\n",
    "[API - randomizedsearchcv](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\r\n",
    "\r\n",
    "```\r\n",
    "parameter_dist_grid = {'n_estimators': reciprocal(20,20000), 'max_features': some_distribution_function}\r\n",
    "\r\n",
    "ms.RandomizedSearchCV(estimator, parameter_dist_grid, \r\n",
    "                        n_iter=10, cv=10, \r\n",
    "                        scoring= 'xxxx', )\r\n",
    "```\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}